[[34m2024-07-13T21:56:21.379+0300[0m] {[34mscheduler_job_runner.py:[0m797} INFO[0m - Starting the scheduler[0m
[[34m2024-07-13T21:56:21.380+0300[0m] {[34mscheduler_job_runner.py:[0m804} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-07-13T21:56:21.638+0300[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 126735[0m
[[34m2024-07-13T21:56:21.640+0300[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-13T21:56:21.648+0300[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2024-07-13T21:57:04.108+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T18:57:03.985665+00:00 [scheduled]>[0m
[[34m2024-07-13T21:57:04.108+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T21:57:04.109+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T18:57:03.985665+00:00 [scheduled]>[0m
[[34m2024-07-13T21:57:04.111+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T21:57:04.112+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T18:57:03.985665+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-07-13T21:57:04.112+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T18:57:03.985665+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T21:57:04.119+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T18:57:03.985665+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T21:57:04.179+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T18:57:03.985665+00:00/task_id=extract_data_sample permission to 509
[[34m2024-07-13T21:57:04.396+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T18:57:03.985665+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T21:57:12.027+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T18:57:03.985665+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T21:57:12.035+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data_sample, run_id=manual__2024-07-13T18:57:03.985665+00:00, map_index=-1, run_start_date=2024-07-13 18:57:04.575717+00:00, run_end_date=2024-07-13 18:57:11.441063+00:00, run_duration=6.865346, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=3, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-07-13 18:57:04.109689+00:00, queued_by_job_id=1, pid=127262[0m
[[34m2024-07-13T22:01:21.793+0300[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-13T22:02:12.161+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T18:57:03.985665+00:00 [scheduled]>[0m
[[34m2024-07-13T22:02:12.162+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:02:12.162+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T18:57:03.985665+00:00 [scheduled]>[0m
[[34m2024-07-13T22:02:12.170+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T18:57:03.985665+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-07-13T22:02:12.171+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T18:57:03.985665+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:02:12.227+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T18:57:03.985665+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:02:12.535+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
[[34m2024-07-13T22:02:12.937+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T18:57:03.985665+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:02:21.272+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T18:57:03.985665+00:00', try_number=2, map_index=-1)[0m
[[34m2024-07-13T22:02:21.279+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data_sample, run_id=manual__2024-07-13T18:57:03.985665+00:00, map_index=-1, run_start_date=2024-07-13 19:02:13.154429+00:00, run_end_date=2024-07-13 19:02:20.621678+00:00, run_duration=7.467249, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=4, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-07-13 19:02:12.163492+00:00, queued_by_job_id=1, pid=130031[0m
[[34m2024-07-13T22:02:22.055+0300[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-13 18:57:03.985665+00:00: manual__2024-07-13T18:57:03.985665+00:00, state:running, queued_at: 2024-07-13 18:57:04.005121+00:00. externally triggered: True> failed[0m
[[34m2024-07-13T22:02:22.071+0300[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-13 18:57:03.985665+00:00, run_id=manual__2024-07-13T18:57:03.985665+00:00, run_start_date=2024-07-13 18:57:04.059536+00:00, run_end_date=2024-07-13 19:02:22.068591+00:00, run_duration=318.009055, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-13 18:57:03.985665+00:00, data_interval_end=2024-07-13 18:57:03.985665+00:00, dag_hash=2e301d621b7b9608ece0fca0b29075fb[0m
[[34m2024-07-13T22:04:49.961+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:04:49.729676+00:00 [scheduled]>[0m
[[34m2024-07-13T22:04:49.963+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:04:49.963+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:04:49.729676+00:00 [scheduled]>[0m
[[34m2024-07-13T22:04:49.965+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:04:49.967+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:04:49.729676+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-07-13T22:04:49.967+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:04:49.729676+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:04:50.020+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:04:49.729676+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:04:50.190+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:04:49.729676+00:00/task_id=extract_data_sample permission to 509
[[34m2024-07-13T22:04:50.413+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:04:49.729676+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:05:01.269+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:04:49.729676+00:00 [scheduled]>[0m
[[34m2024-07-13T22:05:01.270+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:05:01.270+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:04:49.729676+00:00 [scheduled]>[0m
[[34m2024-07-13T22:05:01.276+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:05:01.277+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:04:49.729676+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-07-13T22:05:01.278+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:04:49.729676+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:05:01.284+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:04:49.729676+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:05:01.294+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data_sample, run_id=manual__2024-07-13T19:04:49.729676+00:00, map_index=-1, run_start_date=2024-07-13 19:04:50.567757+00:00, run_end_date=2024-07-13 19:05:00.901756+00:00, run_duration=10.333999, state=success, executor_state=success, try_number=1, max_tries=1, job_id=5, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-07-13 19:04:49.964198+00:00, queued_by_job_id=1, pid=131442[0m
[[34m2024-07-13T22:05:01.318+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:04:49.729676+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:05:01.412+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:04:49.729676+00:00/task_id=validate_data_sample permission to 509
[[34m2024-07-13T22:05:01.676+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:04:49.729676+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:05:08.851+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:04:49.729676+00:00 [scheduled]>[0m
[[34m2024-07-13T22:05:08.851+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:05:08.852+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:04:49.729676+00:00 [scheduled]>[0m
[[34m2024-07-13T22:05:08.854+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task version_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:05:08.855+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:04:49.729676+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-13T22:05:08.855+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:04:49.729676+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:05:08.860+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:04:49.729676+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:05:08.866+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data_sample, run_id=manual__2024-07-13T19:04:49.729676+00:00, map_index=-1, run_start_date=2024-07-13 19:05:01.836725+00:00, run_end_date=2024-07-13 19:05:08.492461+00:00, run_duration=6.655736, state=success, executor_state=success, try_number=1, max_tries=1, job_id=6, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-07-13 19:05:01.271489+00:00, queued_by_job_id=1, pid=131591[0m
[[34m2024-07-13T22:05:08.885+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:04:49.729676+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:05:08.978+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:04:49.729676+00:00/task_id=version_data_sample permission to 509
[[34m2024-07-13T22:05:09.179+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:04:49.729676+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:05:13.204+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:04:49.729676+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:05:13.209+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=version_data_sample, run_id=manual__2024-07-13T19:04:49.729676+00:00, map_index=-1, run_start_date=2024-07-13 19:05:09.313568+00:00, run_end_date=2024-07-13 19:05:12.211069+00:00, run_duration=2.897501, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=7, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-07-13 19:05:08.852671+00:00, queued_by_job_id=1, pid=131683[0m
[[34m2024-07-13T22:06:00.922+0300[0m] {[34mscheduler_job_runner.py:[0m1433} INFO[0m - DAG data_extract_dag scheduling was skipped, probably because the DAG record was locked[0m
[[34m2024-07-13T22:06:21.872+0300[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-13T22:06:52.631+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:06:52.468186+00:00 [scheduled]>[0m
[[34m2024-07-13T22:06:52.631+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:06:52.631+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:06:52.468186+00:00 [scheduled]>[0m
[[34m2024-07-13T22:06:52.634+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:06:52.634+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:06:52.468186+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-07-13T22:06:52.635+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:06:52.468186+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:06:52.663+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:06:52.468186+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:06:52.753+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 3
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:06:52.468186+00:00/task_id=extract_data_sample permission to 509
[[34m2024-07-13T22:06:52.997+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:06:52.468186+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:07:01.790+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:06:52.468186+00:00 [scheduled]>[0m
[[34m2024-07-13T22:07:01.790+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:07:01.791+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:06:52.468186+00:00 [scheduled]>[0m
[[34m2024-07-13T22:07:01.794+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:07:01.795+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:06:52.468186+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-07-13T22:07:01.796+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:06:52.468186+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:07:01.801+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:06:52.468186+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:07:01.809+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data_sample, run_id=manual__2024-07-13T19:06:52.468186+00:00, map_index=-1, run_start_date=2024-07-13 19:06:53.134299+00:00, run_end_date=2024-07-13 19:07:00.800599+00:00, run_duration=7.6663, state=success, executor_state=success, try_number=1, max_tries=1, job_id=8, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-07-13 19:06:52.632391+00:00, queued_by_job_id=1, pid=132603[0m
[[34m2024-07-13T22:07:01.823+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:06:52.468186+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:07:01.904+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 3
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:06:52.468186+00:00/task_id=validate_data_sample permission to 509
[[34m2024-07-13T22:07:02.072+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:06:52.468186+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:07:07.932+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:06:52.468186+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:07:07.937+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data_sample, run_id=manual__2024-07-13T19:06:52.468186+00:00, map_index=-1, run_start_date=2024-07-13 19:07:02.179517+00:00, run_end_date=2024-07-13 19:07:07.406744+00:00, run_duration=5.227227, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=9, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-07-13 19:07:01.792267+00:00, queued_by_job_id=1, pid=132725[0m
[[34m2024-07-13T22:07:29.123+0300[0m] {[34mscheduler_job_runner.py:[0m1433} INFO[0m - DAG data_extract_dag scheduling was skipped, probably because the DAG record was locked[0m
[[34m2024-07-13T22:07:29.127+0300[0m] {[34mscheduler_job_runner.py:[0m1433} INFO[0m - DAG data_extract_dag scheduling was skipped, probably because the DAG record was locked[0m
[[34m2024-07-13T22:07:37.069+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:07:36.100419+00:00 [scheduled]>[0m
[[34m2024-07-13T22:07:37.069+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:07:37.069+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:07:36.100419+00:00 [scheduled]>[0m
[[34m2024-07-13T22:07:37.071+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:07:37.072+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:07:36.100419+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-07-13T22:07:37.072+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:07:36.100419+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:07:37.097+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:07:36.100419+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:07:37.193+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 4
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:07:36.100419+00:00/task_id=extract_data_sample permission to 509
[[34m2024-07-13T22:07:37.443+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:07:36.100419+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:07:44.799+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:07:36.100419+00:00 [scheduled]>[0m
[[34m2024-07-13T22:07:44.800+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:07:44.800+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:07:36.100419+00:00 [scheduled]>[0m
[[34m2024-07-13T22:07:44.802+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:07:44.802+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:07:36.100419+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-07-13T22:07:44.802+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:07:36.100419+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:07:44.807+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:07:36.100419+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:07:44.813+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data_sample, run_id=manual__2024-07-13T19:07:36.100419+00:00, map_index=-1, run_start_date=2024-07-13 19:07:37.606388+00:00, run_end_date=2024-07-13 19:07:44.209804+00:00, run_duration=6.603416, state=success, executor_state=success, try_number=1, max_tries=1, job_id=10, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-07-13 19:07:37.070116+00:00, queued_by_job_id=1, pid=133045[0m
[[34m2024-07-13T22:07:44.822+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:07:36.100419+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:07:44.899+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 4
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:07:36.100419+00:00/task_id=validate_data_sample permission to 509
[[34m2024-07-13T22:07:45.110+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:07:36.100419+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:07:51.510+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:07:36.100419+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:07:51.515+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data_sample, run_id=manual__2024-07-13T19:07:36.100419+00:00, map_index=-1, run_start_date=2024-07-13 19:07:45.244417+00:00, run_end_date=2024-07-13 19:07:50.368805+00:00, run_duration=5.124388, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=11, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-07-13 19:07:44.800733+00:00, queued_by_job_id=1, pid=133124[0m
[[34m2024-07-13T22:08:33.364+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:08:32.952237+00:00 [scheduled]>[0m
[[34m2024-07-13T22:08:33.365+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:08:33.365+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:08:32.952237+00:00 [scheduled]>[0m
[[34m2024-07-13T22:08:33.368+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:08:33.369+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:08:32.952237+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-07-13T22:08:33.370+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:08:32.952237+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:08:33.395+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:08:32.952237+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:08:33.476+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 1
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:08:32.952237+00:00/task_id=extract_data_sample permission to 509
[[34m2024-07-13T22:08:33.671+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:08:32.952237+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:08:40.924+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:08:32.952237+00:00 [scheduled]>[0m
[[34m2024-07-13T22:08:40.924+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:08:40.924+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:08:32.952237+00:00 [scheduled]>[0m
[[34m2024-07-13T22:08:40.926+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:08:40.927+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:08:32.952237+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-07-13T22:08:40.927+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:08:32.952237+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:08:40.934+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:08:32.952237+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:08:40.940+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data_sample, run_id=manual__2024-07-13T19:08:32.952237+00:00, map_index=-1, run_start_date=2024-07-13 19:08:33.834740+00:00, run_end_date=2024-07-13 19:08:39.750884+00:00, run_duration=5.916144, state=success, executor_state=success, try_number=1, max_tries=1, job_id=12, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-07-13 19:08:33.366453+00:00, queued_by_job_id=1, pid=133491[0m
[[34m2024-07-13T22:08:40.947+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:08:32.952237+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:08:41.032+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 1
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:08:32.952237+00:00/task_id=validate_data_sample permission to 509
[[34m2024-07-13T22:08:41.224+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:08:32.952237+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:08:47.623+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:08:32.952237+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:08:47.628+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data_sample, run_id=manual__2024-07-13T19:08:32.952237+00:00, map_index=-1, run_start_date=2024-07-13 19:08:41.359824+00:00, run_end_date=2024-07-13 19:08:46.784000+00:00, run_duration=5.424176, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=13, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-07-13 19:08:40.925470+00:00, queued_by_job_id=1, pid=133570[0m
[[34m2024-07-13T22:10:12.556+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:04:49.729676+00:00 [scheduled]>[0m
[[34m2024-07-13T22:10:12.557+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:10:12.557+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:04:49.729676+00:00 [scheduled]>[0m
[[34m2024-07-13T22:10:12.559+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:04:49.729676+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-13T22:10:12.559+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:04:49.729676+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:10:12.584+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:04:49.729676+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:10:12.659+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 1
[[34m2024-07-13T22:10:12.842+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:04:49.729676+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:10:19.314+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:04:49.729676+00:00 [scheduled]>[0m
[[34m2024-07-13T22:10:19.315+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:10:19.315+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:04:49.729676+00:00 [scheduled]>[0m
[[34m2024-07-13T22:10:19.318+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task load_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:10:19.318+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='load_data_sample', run_id='manual__2024-07-13T19:04:49.729676+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-13T22:10:19.319+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'load_data_sample', 'manual__2024-07-13T19:04:49.729676+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:10:19.338+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'load_data_sample', 'manual__2024-07-13T19:04:49.729676+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:10:19.409+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:04:49.729676+00:00', try_number=2, map_index=-1)[0m
[[34m2024-07-13T22:10:19.410+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
[[34m2024-07-13T22:10:19.414+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=version_data_sample, run_id=manual__2024-07-13T19:04:49.729676+00:00, map_index=-1, run_start_date=2024-07-13 19:10:12.952979+00:00, run_end_date=2024-07-13 19:10:19.236380+00:00, run_duration=6.283401, state=success, executor_state=success, try_number=2, max_tries=1, job_id=14, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-07-13 19:10:12.557836+00:00, queued_by_job_id=1, pid=134305[0m
Project stage: 1
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:04:49.729676+00:00/task_id=load_data_sample permission to 509
[[34m2024-07-13T22:10:19.570+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:04:49.729676+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:10:20.502+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='load_data_sample', run_id='manual__2024-07-13T19:04:49.729676+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:10:20.507+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=load_data_sample, run_id=manual__2024-07-13T19:04:49.729676+00:00, map_index=-1, run_start_date=2024-07-13 19:10:19.683401+00:00, run_end_date=2024-07-13 19:10:19.950421+00:00, run_duration=0.26702, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-13 19:10:19.316232+00:00, queued_by_job_id=1, pid=134421[0m
[[34m2024-07-13T22:11:21.959+0300[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-13T22:12:08.306+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:06:52.468186+00:00 [scheduled]>[0m
[[34m2024-07-13T22:12:08.307+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:12:08.307+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:06:52.468186+00:00 [scheduled]>[0m
[[34m2024-07-13T22:12:08.309+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:06:52.468186+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-07-13T22:12:08.310+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:06:52.468186+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:12:08.326+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:06:52.468186+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:12:08.411+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 1
[[34m2024-07-13T22:12:08.586+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:06:52.468186+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:12:14.454+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:06:52.468186+00:00 [scheduled]>[0m
[[34m2024-07-13T22:12:14.454+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:12:14.454+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:06:52.468186+00:00 [scheduled]>[0m
[[34m2024-07-13T22:12:14.456+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task version_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:12:14.457+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:06:52.468186+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-13T22:12:14.457+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:06:52.468186+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:12:14.461+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:06:52.468186+00:00', try_number=2, map_index=-1)[0m
[[34m2024-07-13T22:12:14.466+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data_sample, run_id=manual__2024-07-13T19:06:52.468186+00:00, map_index=-1, run_start_date=2024-07-13 19:12:08.704949+00:00, run_end_date=2024-07-13 19:12:14.088095+00:00, run_duration=5.383146, state=success, executor_state=success, try_number=2, max_tries=1, job_id=16, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-07-13 19:12:08.308229+00:00, queued_by_job_id=1, pid=135395[0m
[[34m2024-07-13T22:12:14.473+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:06:52.468186+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:12:14.559+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 1
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:06:52.468186+00:00/task_id=version_data_sample permission to 509
[[34m2024-07-13T22:12:14.725+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:06:52.468186+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:12:17.672+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:06:52.468186+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:12:17.676+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=version_data_sample, run_id=manual__2024-07-13T19:06:52.468186+00:00, map_index=-1, run_start_date=2024-07-13 19:12:14.844091+00:00, run_end_date=2024-07-13 19:12:16.790729+00:00, run_duration=1.946638, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=17, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-07-13 19:12:14.455273+00:00, queued_by_job_id=1, pid=135474[0m
[[34m2024-07-13T22:12:50.773+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:07:36.100419+00:00 [scheduled]>[0m
[[34m2024-07-13T22:12:50.773+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:12:50.773+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:07:36.100419+00:00 [scheduled]>[0m
[[34m2024-07-13T22:12:50.775+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:07:36.100419+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-07-13T22:12:50.776+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:07:36.100419+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:12:50.795+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:07:36.100419+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:12:50.879+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 1
[[34m2024-07-13T22:12:51.044+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:07:36.100419+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:12:58.688+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:07:36.100419+00:00 [scheduled]>[0m
[[34m2024-07-13T22:12:58.689+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:12:58.689+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:07:36.100419+00:00 [scheduled]>[0m
[[34m2024-07-13T22:12:58.691+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task version_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:12:58.691+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:07:36.100419+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-13T22:12:58.691+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:07:36.100419+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:12:58.695+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:07:36.100419+00:00', try_number=2, map_index=-1)[0m
[[34m2024-07-13T22:12:58.701+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data_sample, run_id=manual__2024-07-13T19:07:36.100419+00:00, map_index=-1, run_start_date=2024-07-13 19:12:51.176545+00:00, run_end_date=2024-07-13 19:12:57.580654+00:00, run_duration=6.404109, state=success, executor_state=success, try_number=2, max_tries=1, job_id=18, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-07-13 19:12:50.774382+00:00, queued_by_job_id=1, pid=135749[0m
[[34m2024-07-13T22:12:58.717+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:07:36.100419+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:12:59.045+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 1
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:07:36.100419+00:00/task_id=version_data_sample permission to 509
[[34m2024-07-13T22:12:59.452+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:07:36.100419+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:13:02.433+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:07:36.100419+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:13:02.438+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=version_data_sample, run_id=manual__2024-07-13T19:07:36.100419+00:00, map_index=-1, run_start_date=2024-07-13 19:12:59.601419+00:00, run_end_date=2024-07-13 19:13:01.615552+00:00, run_duration=2.014133, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=19, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-07-13 19:12:58.689903+00:00, queued_by_job_id=1, pid=135848[0m
[[34m2024-07-13T22:13:47.191+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:08:32.952237+00:00 [scheduled]>[0m
[[34m2024-07-13T22:13:47.192+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:13:47.192+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:08:32.952237+00:00 [scheduled]>[0m
[[34m2024-07-13T22:13:47.195+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:08:32.952237+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-07-13T22:13:47.195+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:08:32.952237+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:13:47.220+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:08:32.952237+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:13:47.301+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 1
[[34m2024-07-13T22:13:47.464+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:08:32.952237+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:13:54.524+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:08:32.952237+00:00 [scheduled]>[0m
[[34m2024-07-13T22:13:54.525+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:13:54.525+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:08:32.952237+00:00 [scheduled]>[0m
[[34m2024-07-13T22:13:54.527+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task version_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:13:54.527+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:08:32.952237+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-13T22:13:54.528+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:08:32.952237+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:13:54.533+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:08:32.952237+00:00', try_number=2, map_index=-1)[0m
[[34m2024-07-13T22:13:54.544+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data_sample, run_id=manual__2024-07-13T19:08:32.952237+00:00, map_index=-1, run_start_date=2024-07-13 19:13:47.580100+00:00, run_end_date=2024-07-13 19:13:53.828296+00:00, run_duration=6.248196, state=success, executor_state=success, try_number=2, max_tries=1, job_id=20, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-07-13 19:13:47.193517+00:00, queued_by_job_id=1, pid=136329[0m
[[34m2024-07-13T22:13:54.548+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:08:32.952237+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:13:54.630+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 1
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:08:32.952237+00:00/task_id=version_data_sample permission to 509
[[34m2024-07-13T22:13:54.792+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:08:32.952237+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:13:57.427+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:08:32.952237+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:13:57.433+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=version_data_sample, run_id=manual__2024-07-13T19:08:32.952237+00:00, map_index=-1, run_start_date=2024-07-13 19:13:54.893222+00:00, run_end_date=2024-07-13 19:13:56.675288+00:00, run_duration=1.782066, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=21, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-07-13 19:13:54.525771+00:00, queued_by_job_id=1, pid=136418[0m
[[34m2024-07-13T22:15:20.976+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:04:49.729676+00:00 [scheduled]>[0m
[[34m2024-07-13T22:15:20.976+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:15:20.977+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:04:49.729676+00:00 [scheduled]>[0m
[[34m2024-07-13T22:15:20.979+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='load_data_sample', run_id='manual__2024-07-13T19:04:49.729676+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-13T22:15:20.979+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'load_data_sample', 'manual__2024-07-13T19:04:49.729676+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:15:21.005+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'load_data_sample', 'manual__2024-07-13T19:04:49.729676+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:15:21.098+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 1
[[34m2024-07-13T22:15:21.282+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:04:49.729676+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:15:22.143+0300[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-13 19:04:49.729676+00:00: manual__2024-07-13T19:04:49.729676+00:00, state:running, queued_at: 2024-07-13 19:04:49.772828+00:00. externally triggered: True> failed[0m
[[34m2024-07-13T22:15:22.154+0300[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-13 19:04:49.729676+00:00, run_id=manual__2024-07-13T19:04:49.729676+00:00, run_start_date=2024-07-13 19:04:49.922003+00:00, run_end_date=2024-07-13 19:15:22.153875+00:00, run_duration=632.231872, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-13 19:04:49.729676+00:00, data_interval_end=2024-07-13 19:04:49.729676+00:00, dag_hash=5d5ca6fcaf17bb1b4c352f1c78071588[0m
[[34m2024-07-13T22:15:22.174+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='load_data_sample', run_id='manual__2024-07-13T19:04:49.729676+00:00', try_number=2, map_index=-1)[0m
[[34m2024-07-13T22:15:22.179+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=load_data_sample, run_id=manual__2024-07-13T19:04:49.729676+00:00, map_index=-1, run_start_date=2024-07-13 19:15:21.424812+00:00, run_end_date=2024-07-13 19:15:21.707803+00:00, run_duration=0.282991, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=22, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-13 19:15:20.977680+00:00, queued_by_job_id=1, pid=137074[0m
[[34m2024-07-13T22:16:22.089+0300[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-13T22:17:16.981+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:06:52.468186+00:00 [scheduled]>[0m
[[34m2024-07-13T22:17:16.982+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:17:16.982+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:06:52.468186+00:00 [scheduled]>[0m
[[34m2024-07-13T22:17:16.985+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:06:52.468186+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-13T22:17:16.986+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:06:52.468186+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:17:17.017+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:06:52.468186+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:17:17.315+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 1
[[34m2024-07-13T22:17:17.778+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:06:52.468186+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:17:20.849+0300[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-13 19:06:52.468186+00:00: manual__2024-07-13T19:06:52.468186+00:00, state:running, queued_at: 2024-07-13 19:06:52.476729+00:00. externally triggered: True> failed[0m
[[34m2024-07-13T22:17:20.849+0300[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-13 19:06:52.468186+00:00, run_id=manual__2024-07-13T19:06:52.468186+00:00, run_start_date=2024-07-13 19:06:52.581461+00:00, run_end_date=2024-07-13 19:17:20.849490+00:00, run_duration=628.268029, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-13 19:06:52.468186+00:00, data_interval_end=2024-07-13 19:06:52.468186+00:00, dag_hash=5d5ca6fcaf17bb1b4c352f1c78071588[0m
[[34m2024-07-13T22:17:20.890+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:06:52.468186+00:00', try_number=2, map_index=-1)[0m
[[34m2024-07-13T22:17:20.894+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=version_data_sample, run_id=manual__2024-07-13T19:06:52.468186+00:00, map_index=-1, run_start_date=2024-07-13 19:17:17.999050+00:00, run_end_date=2024-07-13 19:17:20.472569+00:00, run_duration=2.473519, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=23, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-07-13 19:17:16.983852+00:00, queued_by_job_id=1, pid=138257[0m
[[34m2024-07-13T22:18:02.555+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:07:36.100419+00:00 [scheduled]>[0m
[[34m2024-07-13T22:18:02.555+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:18:02.556+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:07:36.100419+00:00 [scheduled]>[0m
[[34m2024-07-13T22:18:02.557+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:07:36.100419+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-13T22:18:02.558+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:07:36.100419+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:18:02.574+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:07:36.100419+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:18:02.649+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 1
[[34m2024-07-13T22:18:02.842+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:07:36.100419+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:18:05.789+0300[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-13 19:07:36.100419+00:00: manual__2024-07-13T19:07:36.100419+00:00, state:running, queued_at: 2024-07-13 19:07:36.124108+00:00. externally triggered: True> failed[0m
[[34m2024-07-13T22:18:05.790+0300[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-13 19:07:36.100419+00:00, run_id=manual__2024-07-13T19:07:36.100419+00:00, run_start_date=2024-07-13 19:07:37.018739+00:00, run_end_date=2024-07-13 19:18:05.790230+00:00, run_duration=628.771491, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-13 19:07:36.100419+00:00, data_interval_end=2024-07-13 19:07:36.100419+00:00, dag_hash=5d5ca6fcaf17bb1b4c352f1c78071588[0m
[[34m2024-07-13T22:18:05.825+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:07:36.100419+00:00', try_number=2, map_index=-1)[0m
[[34m2024-07-13T22:18:05.829+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=version_data_sample, run_id=manual__2024-07-13T19:07:36.100419+00:00, map_index=-1, run_start_date=2024-07-13 19:18:02.950069+00:00, run_end_date=2024-07-13 19:18:04.769589+00:00, run_duration=1.81952, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=24, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-07-13 19:18:02.556550+00:00, queued_by_job_id=1, pid=138708[0m
[[34m2024-07-13T22:18:31.282+0300[0m] {[34mscheduler_job_runner.py:[0m1433} INFO[0m - DAG data_extract_dag scheduling was skipped, probably because the DAG record was locked[0m
[[34m2024-07-13T22:18:57.200+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:08:32.952237+00:00 [scheduled]>[0m
[[34m2024-07-13T22:18:57.200+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:18:57.201+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:08:32.952237+00:00 [scheduled]>[0m
[[34m2024-07-13T22:18:57.202+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:08:32.952237+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-13T22:18:57.203+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:08:32.952237+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:18:57.219+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:08:32.952237+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:18:57.284+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 1
[[34m2024-07-13T22:18:57.471+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:08:32.952237+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:19:00.373+0300[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-13 19:08:32.952237+00:00: manual__2024-07-13T19:08:32.952237+00:00, state:running, queued_at: 2024-07-13 19:08:32.962598+00:00. externally triggered: True> failed[0m
[[34m2024-07-13T22:19:00.374+0300[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-13 19:08:32.952237+00:00, run_id=manual__2024-07-13T19:08:32.952237+00:00, run_start_date=2024-07-13 19:08:33.271658+00:00, run_end_date=2024-07-13 19:19:00.374777+00:00, run_duration=627.103119, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-13 19:08:32.952237+00:00, data_interval_end=2024-07-13 19:08:32.952237+00:00, dag_hash=5d5ca6fcaf17bb1b4c352f1c78071588[0m
[[34m2024-07-13T22:19:00.406+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:08:32.952237+00:00', try_number=2, map_index=-1)[0m
[[34m2024-07-13T22:19:00.412+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=version_data_sample, run_id=manual__2024-07-13T19:08:32.952237+00:00, map_index=-1, run_start_date=2024-07-13 19:18:57.584135+00:00, run_end_date=2024-07-13 19:18:59.473712+00:00, run_duration=1.889577, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=25, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-07-13 19:18:57.201502+00:00, queued_by_job_id=1, pid=139279[0m
[[34m2024-07-13T22:21:22.140+0300[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-13T22:25:42.205+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:25:41.220744+00:00 [scheduled]>[0m
[[34m2024-07-13T22:25:42.205+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:25:42.206+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:25:41.220744+00:00 [scheduled]>[0m
[[34m2024-07-13T22:25:42.208+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:25:42.208+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:25:41.220744+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-07-13T22:25:42.208+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:25:41.220744+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:25:42.231+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:25:41.220744+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:25:42.320+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:25:41.220744+00:00/task_id=extract_data_sample permission to 509
[[34m2024-07-13T22:25:42.508+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:25:41.220744+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:25:49.419+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:25:41.220744+00:00 [scheduled]>[0m
[[34m2024-07-13T22:25:49.420+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:25:49.420+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:25:41.220744+00:00 [scheduled]>[0m
[[34m2024-07-13T22:25:49.422+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:25:49.423+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:25:41.220744+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-07-13T22:25:49.423+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:25:41.220744+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:25:49.429+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:25:41.220744+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:25:49.437+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data_sample, run_id=manual__2024-07-13T19:25:41.220744+00:00, map_index=-1, run_start_date=2024-07-13 19:25:42.634808+00:00, run_end_date=2024-07-13 19:25:48.815290+00:00, run_duration=6.180482, state=success, executor_state=success, try_number=1, max_tries=1, job_id=26, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-07-13 19:25:42.206767+00:00, queued_by_job_id=1, pid=142688[0m
[[34m2024-07-13T22:25:49.452+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:25:41.220744+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:25:49.635+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:25:41.220744+00:00/task_id=validate_data_sample permission to 509
[[34m2024-07-13T22:25:49.921+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:25:41.220744+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:25:56.353+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:25:41.220744+00:00 [scheduled]>[0m
[[34m2024-07-13T22:25:56.353+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:25:56.353+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:25:41.220744+00:00 [scheduled]>[0m
[[34m2024-07-13T22:25:56.355+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task version_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:25:56.355+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:25:41.220744+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-13T22:25:56.355+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:25:41.220744+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:25:56.360+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:25:41.220744+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:25:56.364+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data_sample, run_id=manual__2024-07-13T19:25:41.220744+00:00, map_index=-1, run_start_date=2024-07-13 19:25:50.058690+00:00, run_end_date=2024-07-13 19:25:56.161849+00:00, run_duration=6.103159, state=success, executor_state=success, try_number=1, max_tries=1, job_id=27, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-07-13 19:25:49.421083+00:00, queued_by_job_id=1, pid=142766[0m
[[34m2024-07-13T22:25:56.373+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:25:41.220744+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:25:56.452+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:25:41.220744+00:00/task_id=version_data_sample permission to 509
[[34m2024-07-13T22:25:56.619+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:25:41.220744+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:26:03.416+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:25:41.220744+00:00 [scheduled]>[0m
[[34m2024-07-13T22:26:03.417+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:26:03.418+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:25:41.220744+00:00 [scheduled]>[0m
[[34m2024-07-13T22:26:03.421+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task load_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:26:03.422+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='load_data_sample', run_id='manual__2024-07-13T19:25:41.220744+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-13T22:26:03.422+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'load_data_sample', 'manual__2024-07-13T19:25:41.220744+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:26:03.428+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:25:41.220744+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:26:03.434+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=version_data_sample, run_id=manual__2024-07-13T19:25:41.220744+00:00, map_index=-1, run_start_date=2024-07-13 19:25:56.735412+00:00, run_end_date=2024-07-13 19:26:02.902389+00:00, run_duration=6.166977, state=success, executor_state=success, try_number=1, max_tries=1, job_id=28, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-07-13 19:25:56.353975+00:00, queued_by_job_id=1, pid=142847[0m
[[34m2024-07-13T22:26:03.443+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'load_data_sample', 'manual__2024-07-13T19:25:41.220744+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:26:03.519+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:25:41.220744+00:00/task_id=load_data_sample permission to 509
[[34m2024-07-13T22:26:03.704+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:25:41.220744+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:26:04.550+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='load_data_sample', run_id='manual__2024-07-13T19:25:41.220744+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:26:04.554+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=load_data_sample, run_id=manual__2024-07-13T19:25:41.220744+00:00, map_index=-1, run_start_date=2024-07-13 19:26:03.839325+00:00, run_end_date=2024-07-13 19:26:04.138762+00:00, run_duration=0.299437, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=29, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-13 19:26:03.419400+00:00, queued_by_job_id=1, pid=142946[0m
[[34m2024-07-13T22:26:22.206+0300[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-13T22:31:04.865+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:25:41.220744+00:00 [scheduled]>[0m
[[34m2024-07-13T22:31:04.866+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:31:04.866+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:25:41.220744+00:00 [scheduled]>[0m
[[34m2024-07-13T22:31:04.868+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='load_data_sample', run_id='manual__2024-07-13T19:25:41.220744+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-13T22:31:04.869+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'load_data_sample', 'manual__2024-07-13T19:25:41.220744+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:31:04.882+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'load_data_sample', 'manual__2024-07-13T19:25:41.220744+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:31:04.954+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
[[34m2024-07-13T22:31:05.128+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:25:41.220744+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:31:06.175+0300[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-13 19:25:41.220744+00:00: manual__2024-07-13T19:25:41.220744+00:00, state:running, queued_at: 2024-07-13 19:25:41.230340+00:00. externally triggered: True> failed[0m
[[34m2024-07-13T22:31:06.177+0300[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-13 19:25:41.220744+00:00, run_id=manual__2024-07-13T19:25:41.220744+00:00, run_start_date=2024-07-13 19:25:42.175777+00:00, run_end_date=2024-07-13 19:31:06.176465+00:00, run_duration=324.000688, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-13 19:25:41.220744+00:00, data_interval_end=2024-07-13 19:25:41.220744+00:00, dag_hash=2e301d621b7b9608ece0fca0b29075fb[0m
[[34m2024-07-13T22:31:06.206+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='load_data_sample', run_id='manual__2024-07-13T19:25:41.220744+00:00', try_number=2, map_index=-1)[0m
[[34m2024-07-13T22:31:06.212+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=load_data_sample, run_id=manual__2024-07-13T19:25:41.220744+00:00, map_index=-1, run_start_date=2024-07-13 19:31:05.230160+00:00, run_end_date=2024-07-13 19:31:05.456357+00:00, run_duration=0.226197, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=30, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-13 19:31:04.867495+00:00, queued_by_job_id=1, pid=145696[0m
[[34m2024-07-13T22:31:22.250+0300[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-13T22:36:22.291+0300[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-13T22:41:20.817+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:41:20.709872+00:00 [scheduled]>[0m
[[34m2024-07-13T22:41:20.817+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:41:20.817+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:41:20.709872+00:00 [scheduled]>[0m
[[34m2024-07-13T22:41:20.819+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:41:20.820+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:41:20.709872+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-07-13T22:41:20.820+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:41:20.709872+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:41:20.840+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:41:20.709872+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:41:20.928+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:41:20.709872+00:00/task_id=extract_data_sample permission to 509
[[34m2024-07-13T22:41:21.173+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:41:20.709872+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:41:22.369+0300[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-13T22:41:28.809+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:41:20.709872+00:00 [scheduled]>[0m
[[34m2024-07-13T22:41:28.809+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:41:28.810+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:41:20.709872+00:00 [scheduled]>[0m
[[34m2024-07-13T22:41:28.812+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:41:28.812+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:41:20.709872+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-07-13T22:41:28.813+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:41:20.709872+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:41:28.817+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:41:20.709872+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:41:28.824+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data_sample, run_id=manual__2024-07-13T19:41:20.709872+00:00, map_index=-1, run_start_date=2024-07-13 19:41:21.300723+00:00, run_end_date=2024-07-13 19:41:27.727169+00:00, run_duration=6.426446, state=success, executor_state=success, try_number=1, max_tries=1, job_id=31, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-07-13 19:41:20.818357+00:00, queued_by_job_id=1, pid=150488[0m
[[34m2024-07-13T22:41:28.859+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:41:20.709872+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:41:29.051+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:41:20.709872+00:00/task_id=validate_data_sample permission to 509
[[34m2024-07-13T22:41:29.314+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:41:20.709872+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:41:37.278+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:41:20.709872+00:00 [scheduled]>[0m
[[34m2024-07-13T22:41:37.278+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:41:37.279+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:41:20.709872+00:00 [scheduled]>[0m
[[34m2024-07-13T22:41:37.282+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task version_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:41:37.282+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:41:20.709872+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-13T22:41:37.283+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:41:20.709872+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:41:37.287+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:41:20.709872+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:41:37.293+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data_sample, run_id=manual__2024-07-13T19:41:20.709872+00:00, map_index=-1, run_start_date=2024-07-13 19:41:29.453584+00:00, run_end_date=2024-07-13 19:41:36.436348+00:00, run_duration=6.982764, state=success, executor_state=success, try_number=1, max_tries=1, job_id=32, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-07-13 19:41:28.810612+00:00, queued_by_job_id=1, pid=150576[0m
[[34m2024-07-13T22:41:37.337+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:41:20.709872+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:41:37.517+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:41:20.709872+00:00/task_id=version_data_sample permission to 509
[[34m2024-07-13T22:41:37.716+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:41:20.709872+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:41:44.466+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:41:20.709872+00:00 [scheduled]>[0m
[[34m2024-07-13T22:41:44.466+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:41:44.467+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:41:20.709872+00:00 [scheduled]>[0m
[[34m2024-07-13T22:41:44.469+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task load_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:41:44.470+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='load_data_sample', run_id='manual__2024-07-13T19:41:20.709872+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-13T22:41:44.470+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'load_data_sample', 'manual__2024-07-13T19:41:20.709872+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:41:44.476+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:41:20.709872+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:41:44.482+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=version_data_sample, run_id=manual__2024-07-13T19:41:20.709872+00:00, map_index=-1, run_start_date=2024-07-13 19:41:37.912675+00:00, run_end_date=2024-07-13 19:41:44.280850+00:00, run_duration=6.368175, state=success, executor_state=success, try_number=1, max_tries=1, job_id=33, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-07-13 19:41:37.280643+00:00, queued_by_job_id=1, pid=150675[0m
[[34m2024-07-13T22:41:44.521+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'load_data_sample', 'manual__2024-07-13T19:41:20.709872+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:41:44.657+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:41:20.709872+00:00/task_id=load_data_sample permission to 509
[[34m2024-07-13T22:41:44.833+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:41:20.709872+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:41:45.596+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='load_data_sample', run_id='manual__2024-07-13T19:41:20.709872+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:41:45.601+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=load_data_sample, run_id=manual__2024-07-13T19:41:20.709872+00:00, map_index=-1, run_start_date=2024-07-13 19:41:44.955931+00:00, run_end_date=2024-07-13 19:41:45.226011+00:00, run_duration=0.27008, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=34, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-13 19:41:44.468356+00:00, queued_by_job_id=1, pid=150768[0m
[[34m2024-07-13T22:45:18.286+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:45:17.968623+00:00 [scheduled]>[0m
[[34m2024-07-13T22:45:18.286+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:45:18.286+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:45:17.968623+00:00 [scheduled]>[0m
[[34m2024-07-13T22:45:18.289+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:45:18.290+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:45:17.968623+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-07-13T22:45:18.291+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:45:17.968623+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:45:18.320+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:45:17.968623+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:45:18.472+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:45:17.968623+00:00/task_id=extract_data_sample permission to 509
[[34m2024-07-13T22:45:18.696+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:45:17.968623+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:45:25.776+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:45:17.968623+00:00 [scheduled]>[0m
[[34m2024-07-13T22:45:25.776+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:45:25.776+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:45:17.968623+00:00 [scheduled]>[0m
[[34m2024-07-13T22:45:25.779+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:45:25.779+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:45:17.968623+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-07-13T22:45:25.779+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:45:17.968623+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:45:25.785+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:45:17.968623+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:45:25.791+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data_sample, run_id=manual__2024-07-13T19:45:17.968623+00:00, map_index=-1, run_start_date=2024-07-13 19:45:18.878644+00:00, run_end_date=2024-07-13 19:45:24.695898+00:00, run_duration=5.817254, state=success, executor_state=success, try_number=1, max_tries=1, job_id=35, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-07-13 19:45:18.287439+00:00, queued_by_job_id=1, pid=152449[0m
[[34m2024-07-13T22:45:25.804+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:45:17.968623+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:45:25.934+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:45:17.968623+00:00/task_id=validate_data_sample permission to 509
[[34m2024-07-13T22:45:26.161+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:45:17.968623+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:45:33.385+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:45:17.968623+00:00 [scheduled]>[0m
[[34m2024-07-13T22:45:33.386+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:45:33.386+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:45:17.968623+00:00 [scheduled]>[0m
[[34m2024-07-13T22:45:33.388+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task version_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:45:33.388+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:45:17.968623+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-13T22:45:33.388+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:45:17.968623+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:45:33.394+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:45:17.968623+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:45:33.400+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data_sample, run_id=manual__2024-07-13T19:45:17.968623+00:00, map_index=-1, run_start_date=2024-07-13 19:45:26.295850+00:00, run_end_date=2024-07-13 19:45:32.530505+00:00, run_duration=6.234655, state=success, executor_state=success, try_number=1, max_tries=1, job_id=36, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-07-13 19:45:25.777664+00:00, queued_by_job_id=1, pid=152539[0m
[[34m2024-07-13T22:45:33.404+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:45:17.968623+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:45:33.513+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:45:17.968623+00:00/task_id=version_data_sample permission to 509
[[34m2024-07-13T22:45:33.766+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:45:17.968623+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:45:36.563+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:45:17.968623+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:45:36.570+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=version_data_sample, run_id=manual__2024-07-13T19:45:17.968623+00:00, map_index=-1, run_start_date=2024-07-13 19:45:33.908126+00:00, run_end_date=2024-07-13 19:45:36.200193+00:00, run_duration=2.292067, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=37, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-07-13 19:45:33.386754+00:00, queued_by_job_id=1, pid=152634[0m
[[34m2024-07-13T22:46:22.438+0300[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-13T22:46:45.400+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:41:20.709872+00:00 [scheduled]>[0m
[[34m2024-07-13T22:46:45.401+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:46:45.401+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:41:20.709872+00:00 [scheduled]>[0m
[[34m2024-07-13T22:46:45.403+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='load_data_sample', run_id='manual__2024-07-13T19:41:20.709872+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-13T22:46:45.403+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'load_data_sample', 'manual__2024-07-13T19:41:20.709872+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:46:45.419+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'load_data_sample', 'manual__2024-07-13T19:41:20.709872+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:46:45.536+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
[[34m2024-07-13T22:46:45.706+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.load_data_sample manual__2024-07-13T19:41:20.709872+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:46:46.524+0300[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-13 19:41:20.709872+00:00: manual__2024-07-13T19:41:20.709872+00:00, state:running, queued_at: 2024-07-13 19:41:20.718755+00:00. externally triggered: True> failed[0m
[[34m2024-07-13T22:46:46.524+0300[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-13 19:41:20.709872+00:00, run_id=manual__2024-07-13T19:41:20.709872+00:00, run_start_date=2024-07-13 19:41:20.789166+00:00, run_end_date=2024-07-13 19:46:46.524914+00:00, run_duration=325.735748, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-13 19:41:20.709872+00:00, data_interval_end=2024-07-13 19:41:20.709872+00:00, dag_hash=089c8199427143d65a82adc24103dacb[0m
[[34m2024-07-13T22:46:46.548+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='load_data_sample', run_id='manual__2024-07-13T19:41:20.709872+00:00', try_number=2, map_index=-1)[0m
[[34m2024-07-13T22:46:46.554+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=load_data_sample, run_id=manual__2024-07-13T19:41:20.709872+00:00, map_index=-1, run_start_date=2024-07-13 19:46:45.814768+00:00, run_end_date=2024-07-13 19:46:46.083913+00:00, run_duration=0.269145, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=38, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-13 19:46:45.402220+00:00, queued_by_job_id=1, pid=153189[0m
[[34m2024-07-13T22:47:10.787+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:47:10.256471+00:00 [scheduled]>[0m
[[34m2024-07-13T22:47:10.787+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:47:10.787+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:47:10.256471+00:00 [scheduled]>[0m
[[34m2024-07-13T22:47:10.789+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:47:10.790+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:47:10.256471+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-07-13T22:47:10.790+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:47:10.256471+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:47:10.806+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data_sample', 'manual__2024-07-13T19:47:10.256471+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:47:10.934+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:47:10.256471+00:00/task_id=extract_data_sample permission to 509
[[34m2024-07-13T22:47:11.195+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data_sample manual__2024-07-13T19:47:10.256471+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:47:16.974+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:47:10.256471+00:00 [scheduled]>[0m
[[34m2024-07-13T22:47:16.974+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:47:16.975+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:47:10.256471+00:00 [scheduled]>[0m
[[34m2024-07-13T22:47:16.977+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:47:16.978+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:47:10.256471+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-07-13T22:47:16.978+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:47:10.256471+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:47:16.996+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data_sample', 'manual__2024-07-13T19:47:10.256471+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:47:17.058+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data_sample', run_id='manual__2024-07-13T19:47:10.256471+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:47:17.063+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data_sample, run_id=manual__2024-07-13T19:47:10.256471+00:00, map_index=-1, run_start_date=2024-07-13 19:47:11.345244+00:00, run_end_date=2024-07-13 19:47:16.899558+00:00, run_duration=5.554314, state=success, executor_state=success, try_number=1, max_tries=1, job_id=39, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2024-07-13 19:47:10.788267+00:00, queued_by_job_id=1, pid=153389[0m
[[34m2024-07-13T22:47:17.148+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:47:10.256471+00:00/task_id=validate_data_sample permission to 509
[[34m2024-07-13T22:47:17.342+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data_sample manual__2024-07-13T19:47:10.256471+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:47:24.212+0300[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:47:10.256471+00:00 [scheduled]>[0m
[[34m2024-07-13T22:47:24.212+0300[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-13T22:47:24.213+0300[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:47:10.256471+00:00 [scheduled]>[0m
[[34m2024-07-13T22:47:24.215+0300[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task version_data_sample because previous state change time has not been saved[0m
[[34m2024-07-13T22:47:24.215+0300[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:47:10.256471+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-13T22:47:24.216+0300[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:47:10.256471+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:47:24.220+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data_sample', run_id='manual__2024-07-13T19:47:10.256471+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:47:24.226+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data_sample, run_id=manual__2024-07-13T19:47:10.256471+00:00, map_index=-1, run_start_date=2024-07-13 19:47:17.466862+00:00, run_end_date=2024-07-13 19:47:23.363546+00:00, run_duration=5.896684, state=success, executor_state=success, try_number=1, max_tries=1, job_id=40, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-07-13 19:47:16.975706+00:00, queued_by_job_id=1, pid=153464[0m
[[34m2024-07-13T22:47:24.235+0300[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'data_extract_dag', 'version_data_sample', 'manual__2024-07-13T19:47:10.256471+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-13T22:47:24.358+0300[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/sshk/project/services/airflow/dags/data_extract_dag.py[0m
Project stage: 2
Changing /home/sshk/project/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-13T19:47:10.256471+00:00/task_id=version_data_sample permission to 509
[[34m2024-07-13T22:47:24.562+0300[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.version_data_sample manual__2024-07-13T19:47:10.256471+00:00 [queued]> on host kwa.[0m
[[34m2024-07-13T22:47:27.137+0300[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='version_data_sample', run_id='manual__2024-07-13T19:47:10.256471+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-13T22:47:27.142+0300[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=version_data_sample, run_id=manual__2024-07-13T19:47:10.256471+00:00, map_index=-1, run_start_date=2024-07-13 19:47:24.687673+00:00, run_end_date=2024-07-13 19:47:26.809459+00:00, run_duration=2.121786, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=41, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-07-13 19:47:24.213395+00:00, queued_by_job_id=1, pid=153564[0m
[[34m2024-07-13T22:48:57.161+0300[0m] {[34mscheduler_job_runner.py:[0m247} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2024-07-13T22:48:58.170+0300[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending 15 to group 126735. PIDs of all processes in the group: [126735][0m
[[34m2024-07-13T22:48:58.171+0300[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal 15 to group 126735[0m
[[34m2024-07-13T22:48:58.345+0300[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=126735, status='terminated', exitcode=0, started='21:56:21') (126735) terminated with exit code 0[0m
[[34m2024-07-13T22:48:58.347+0300[0m] {[34mlocal_executor.py:[0m402} INFO[0m - Shutting down LocalExecutor; waiting for running tasks to finish.  Signal again if you don't want to wait.[0m
[[34m2024-07-13T22:48:58.348+0300[0m] {[34mscheduler_job_runner.py:[0m867} ERROR[0m - Exception when executing Executor.end[0m
Traceback (most recent call last):
  File "/home/sshk/project/.venv/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py", line 844, in _execute
    self._run_scheduler_loop()
  File "/home/sshk/project/.venv/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py", line 999, in _run_scheduler_loop
    time.sleep(min(self._scheduler_idle_sleep_time, next_event if next_event else 0))
  File "/home/sshk/project/.venv/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py", line 250, in _exit_gracefully
    sys.exit(os.EX_OK)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sshk/project/.venv/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py", line 865, in _execute
    self.job.executor.end()
  File "/home/sshk/project/.venv/lib/python3.11/site-packages/airflow/executors/local_executor.py", line 406, in end
    self.impl.end()
  File "/home/sshk/project/.venv/lib/python3.11/site-packages/airflow/executors/local_executor.py", line 350, in end
    self.queue.put((None, None))
  File "<string>", line 2, in put
  File "/usr/lib/python3.11/multiprocessing/managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "/usr/lib/python3.11/multiprocessing/connection.py", line 210, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "/usr/lib/python3.11/multiprocessing/connection.py", line 415, in _send_bytes
    self._send(header + buf)
  File "/usr/lib/python3.11/multiprocessing/connection.py", line 372, in _send
    n = write(self._handle, buf)
        ^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe[0m
[[34m2024-07-13T22:48:58.368+0300[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending 15 to group 126735. PIDs of all processes in the group: [][0m
[[34m2024-07-13T22:48:58.369+0300[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal 15 to group 126735[0m
[[34m2024-07-13T22:48:58.370+0300[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending the signal 15 to process 126735 as process group is missing.[0m
[[34m2024-07-13T22:48:58.370+0300[0m] {[34mscheduler_job_runner.py:[0m873} INFO[0m - Exited execute loop[0m
